{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiV7a8ZY9Y7J"
      },
      "source": [
        "# GENESIS Level 0 Training\n",
        "\n",
        "Train the Level 0 (Machine Code Patterns) model using Google Colab's free GPU.\n",
        "\n",
        "## Setup\n",
        "1. Go to **Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**\n",
        "2. Run all cells in order (Ctrl+F9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WRLi02T29Y7K",
        "outputId": "81d0fad8-65e2-4a23-9d38-68a96da4905e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name, memory.total [MiB]\n",
            "Tesla T4, 15360 MiB\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Check GPU availability\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6Rp6DQa29Y7K",
        "outputId": "5b55bb20-dfcc-4fa8-ee95-4105eb7762ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'genesis'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 100 (delta 11), reused 98 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (100/100), 128.96 KiB | 1.77 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n",
            "Working directory: /content/genesis/genesis/genesis/genesis/genesis\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Clone the repository\n",
        "import os\n",
        "\n",
        "# Remove if exists and clone fresh\n",
        "if os.path.exists('genesis'):\n",
        "    !rm -rf genesis\n",
        "\n",
        "!git clone https://github.com/0xMayoor/genesis.git\n",
        "os.chdir('genesis')\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Z2Ixw2_29Y7L"
      },
      "outputs": [],
      "source": [
        "# Step 3: Install dependencies\n",
        "!pip install -q torch transformers peft accelerate capstone hypothesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "M6Ibzjvu9Y7L",
        "outputId": "842b4737-0281-4bb7-c69e-29d5ec7ca363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for genesis (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Install GENESIS package\n",
        "!pip install -e . --force-reinstall --no-deps -q\n",
        "print(\"Installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ndIpNIsY9Y7L",
        "outputId": "1d555e1b-1d3c-4b87-98a2-f5ff63a2920e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "VRAM: 15.8 GB\n",
            "\n",
            "‚úÖ All imports working!\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Verify installation\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add current directory to path\n",
        "cwd = os.getcwd()\n",
        "if cwd not in sys.path:\n",
        "    sys.path.insert(0, cwd)\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Test imports\n",
        "from core.training import TrainingConfig, ModelConfig\n",
        "from levels.level0_machine import Level0Module\n",
        "print(\"\\n‚úÖ All imports working!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "FQRZLast9Y7L",
        "outputId": "980f5a3f-0576-42ce-dbc7-8febbca99a53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset exists: genesis_datasets/level0/train.jsonl\n",
            "   Samples: 1493\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Check dataset\n",
        "from pathlib import Path\n",
        "\n",
        "dataset_path = Path(\"genesis_datasets/level0/train.jsonl\")\n",
        "if dataset_path.exists():\n",
        "    line_count = sum(1 for _ in open(dataset_path))\n",
        "    print(f\"‚úÖ Dataset exists: {dataset_path}\")\n",
        "    print(f\"   Samples: {line_count}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Dataset not found, generating...\")\n",
        "    from genesis_datasets.generators.level0_generator import Level0DatasetGenerator\n",
        "\n",
        "    generator = Level0DatasetGenerator(seed=42)\n",
        "    samples = generator.generate_dataset(\n",
        "        synthetic_count=5000,\n",
        "        adversarial_count=1000,\n",
        "        binary_paths=[],\n",
        "    )\n",
        "    dataset_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    generator.save_dataset(samples, dataset_path)\n",
        "    print(f\"‚úÖ Generated {len(samples)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "RKfcPbyy9Y7M",
        "outputId": "12b007d1-c1a4-4bca-9376-48e703691351",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Configuration:\n",
            "  Model: distilgpt2\n",
            "  LoRA rank: 8\n",
            "  Batch size: 8\n",
            "  Epochs: 3\n",
            "  FP16: True\n",
            "\n",
            "‚úÖ Configuration valid!\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Configure training\n",
        "from pathlib import Path\n",
        "from core.training import TrainingConfig, ModelConfig\n",
        "\n",
        "config = TrainingConfig(\n",
        "    output_dir=Path(\"models/level0\"),\n",
        "    model=ModelConfig(\n",
        "        model_name=\"distilgpt2\",\n",
        "        use_lora=True,\n",
        "        lora_r=8,\n",
        "        max_length=256,\n",
        "    ),\n",
        "    batch_size=8,\n",
        "    num_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        ")\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"  Model: {config.model.model_name}\")\n",
        "print(f\"  LoRA rank: {config.model.lora_r}\")\n",
        "print(f\"  Batch size: {config.batch_size}\")\n",
        "print(f\"  Epochs: {config.num_epochs}\")\n",
        "print(f\"  FP16: {config.fp16}\")\n",
        "\n",
        "warnings = config.validate()\n",
        "if warnings:\n",
        "    for w in warnings:\n",
        "        print(f\"‚ö†Ô∏è {w}\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Configuration valid!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "uTKa29J99Y7M",
        "outputId": "882a1cdf-0a8a-426d-e88c-1d0eb458d9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "This will take ~10-30 minutes on a T4 GPU.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Target modules {'query', 'value'} not found in the base model. Please check the target modules and try again.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4011898594.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_level0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/genesis/core/training/trainer.py\u001b[0m in \u001b[0;36mtrain_level0\u001b[0;34m(config, train_path)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"genesis_datasets/level0/train.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLevel0Trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/genesis/core/training/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Initialize model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/genesis/core/training/trainer.py\u001b[0m in \u001b[0;36m_init_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m             )\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_trainable_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py\u001b[0m in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    120\u001b[0m         )\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"default\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m     ) -> None:\n\u001b[0;32m-> 1885\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1886\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prepare_inputs_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_empty_weights\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cast_adapter_dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name, low_cpu_mem_usage, state_dict)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXLORA\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXLORA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;31m# Copy the peft_config in the injected model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36minject_adapter\u001b[0;34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage, state_dict)\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"layers_pattern\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m                     \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" You also specified 'layers_pattern': {peft_config.layers_pattern}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m                 \u001b[0;31m# Some modules did not match and some matched but were excluded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target modules {'query', 'value'} not found in the base model. Please check the target modules and try again."
          ]
        }
      ],
      "source": [
        "# Step 8: Train the model!\n",
        "from core.training import train_level0\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"This will take ~10-30 minutes on a T4 GPU.\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "metrics = train_level0(config, dataset_path)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéâ Training Complete!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_66GOmGf9Y7M"
      },
      "outputs": [],
      "source": [
        "# Step 9: Check results\n",
        "print(\"Training Metrics:\")\n",
        "print(f\"  Accuracy: {metrics.accuracy:.2%}\")\n",
        "print(f\"  Adversarial Refusal Rate: {metrics.adversarial_refusal_rate:.2%}\")\n",
        "print(f\"  False Positive Rate: {metrics.false_positive_rate:.2%}\")\n",
        "print(f\"  Total Samples: {metrics.total_samples}\")\n",
        "\n",
        "print(\"\\nGate Requirements:\")\n",
        "passes, failures = metrics.meets_gate_requirements()\n",
        "\n",
        "if passes:\n",
        "    print(\"‚úÖ Model PASSES all gate requirements!\")\n",
        "    print(\"üöÄ Level 0 complete. Ready for Level 1 (Assembly).\")\n",
        "else:\n",
        "    print(\"‚ùå Model FAILS gate requirements:\")\n",
        "    for f in failures:\n",
        "        print(f\"  - {f}\")\n",
        "    print(\"\\nConsider: more epochs, larger dataset, or hyperparameter tuning.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o91O4Tur9Y7M"
      },
      "outputs": [],
      "source": [
        "# Step 10: Download trained model\n",
        "!zip -r level0_model.zip models/level0/\n",
        "\n",
        "from google.colab import files\n",
        "files.download('level0_model.zip')\n",
        "\n",
        "print(\"\\nüì¶ Model downloaded! Extract and place in your local genesis/models/level0/ folder.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}